name: Model evaluation
inputs:
- {name: test_loss}
- {name: test_acc}
- {name: export_path}
outputs:
- {name: mlpipeline_metrics, type: Metrics}
implementation:
  container:
    image: python:3.7
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def model_evaluation(test_loss, test_acc, export_path):\n    #import libraries\n\
      \    import sys, subprocess;\n    subprocess.run([sys.executable, '-m', 'pip',\
      \ 'install', 'tensorflow==2.0.0b0'])\n    import tensorflow as tf\n    from\
      \ tensorflow.python.lib.io import file_io\n    import json\n    from collections\
      \ import namedtuple\n\n    #Creating Metrics\n    print(\"Creating JSON dump\"\
      )\n    metrics = {\n    'metrics': [\n        {\n          'name': 'loss-value',\
      \ # The name of the metric. Visualized as the column name in the runs table.\n\
      \          'numberValue': test_loss, # The value of the metric. Must be a numeric\
      \ value.\n          'format': \"RAW\"   # The optional format of the metric.\
      \ Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\"\
      \ (displayed in percentage format).\n        },\n        {\n          'name':\
      \ 'accuracy-score', # The name of the metric. Visualized as the column name\
      \ in the runs table.\n          'numberValue': test_acc, # The value of the\
      \ metric. Must be a numeric value.\n          'format': \"PERCENTAGE\" \n  \
      \      }]\n    }\n    #Write JSON dump file\n    with file_io.FileIO('/mlpipeline-metrics.json',\
      \ 'w') as f:\n        json.dump(metrics, f)\n\n    print(\"JSON dump done\"\
      )\n\n    return(json.dumps(metrics))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Model\
      \ evaluation', description='')\n_parser.add_argument(\"--test-loss\", dest=\"\
      test_loss\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --test-acc\", dest=\"test_acc\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--export-path\", dest=\"export_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
      _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = model_evaluation(**_parsed_args)\n\
      \n_output_serializers = [\n    str,\n\n]\n\nimport os\nfor idx, output_file\
      \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --test-loss
    - {inputValue: test_loss}
    - --test-acc
    - {inputValue: test_acc}
    - --export-path
    - {inputValue: export_path}
    - '----output-paths'
    - {outputPath: mlpipeline_metrics}
