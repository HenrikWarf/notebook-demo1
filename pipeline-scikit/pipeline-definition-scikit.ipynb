{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modified-sodium",
   "metadata": {},
   "source": [
    "# Vertex AI Pipeline for Scikit-learn Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-display",
   "metadata": {},
   "source": [
    "### Set up project definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collaborative-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  crazy-hippo-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"crazy-hippo-01\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-perth",
   "metadata": {},
   "source": [
    "### Set up current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dressed-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp:  20210625152839\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print('Timestamp: ', TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-commission",
   "metadata": {},
   "source": [
    "### Import Libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comic-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://crazy-vertex-ai-pipelines\"\n",
    "REGION = \"us-central1\" \n",
    "ML_PROJECT_NAME = \"scikit-learn-classifier\"\n",
    "USER = \"crazy-hippo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preliminary-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://crazy-vertex-ai-pipelines/scikit-learn-classifier/crazy-hippo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "PIPELINE_ROOT = \"{}/{}/{}\".format(BUCKET_NAME, ML_PROJECT_NAME, USER)\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "billion-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    InputPath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-renewal",
   "metadata": {},
   "source": [
    "### Define Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-symbol",
   "metadata": {},
   "source": [
    "#### 1. Pre-processing Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hidden-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='pre-processing.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'google-cloud-bigquery', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs',\n",
    "                             'numpy',\n",
    "                             'sklearn'\n",
    "                              ])\n",
    "def pre_processing(\n",
    "        X_TRAIN : Output[Dataset],\n",
    "        X_TEST : Output[Dataset],\n",
    "        y_TRAIN : Output[Dataset],\n",
    "        y_TEST : Output[Dataset],\n",
    "        #pipeline_metrics: Output[Metrics]) -> NamedTuple(\n",
    "        #  'ComponentOutputs',\n",
    "        #  [\n",
    "        #      ('xxxx', str),\n",
    "        #  ]\n",
    "    ):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import sklearn as sk\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client(project='crazy-hippo-01')\n",
    "    \n",
    "    query = \"\"\"SELECT *\n",
    "    FROM `crazy-hippo-01.census_data_us.census_raw` \n",
    "    \"\"\"\n",
    "    \n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    df = job.to_dataframe()\n",
    "    \n",
    "    #Make Feature Selections\n",
    "    X = df[['age', 'workclass', 'gender', 'occupation', 'education_num', 'marital_status', 'relationship', 'capital_gain']]\n",
    "    y = df[['income_bracket']]\n",
    "    \n",
    "    #One-hot encode data using Pandas get_dummies function\n",
    "    X = pd.get_dummies(X, prefix=['workclass', 'gender','occupation','marital_status','relationship'])\n",
    "\n",
    "    #Normalize data using Scikit-learn function\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X[['age','education_num','capital_gain']] = scaler.fit_transform(X[['age','education_num','capital_gain']])\n",
    "    \n",
    "    # Change label string into integer to be able to use in model training\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y['income_bracket'])\n",
    "    y['income_bracket'] = le.transform(y['income_bracket'])\n",
    "    y = y['income_bracket'].values\n",
    "    \n",
    "    #Split data in train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    y_train = pd.DataFrame(y_train, columns=['income_bracket'])\n",
    "    y_test = pd.DataFrame(y_train, columns=['income_bracket'])\n",
    "    \n",
    "    #Write dataframes to CSV artifact and store in GCS\n",
    "    X_train.to_csv(X_TRAIN.path, index=False, header=True)\n",
    "    X_test.to_csv(X_TEST.path, index=False, header=True)\n",
    "    y_train.to_csv(y_TRAIN.path, index=False, header=True)\n",
    "    y_test.to_csv(y_TEST.path, index=False, header=True)\n",
    "    \n",
    "    print(\"Artifacts written to Artifact Repository\")\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-annual",
   "metadata": {},
   "source": [
    "#### 2. Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dated-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='training.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'google-cloud-bigquery', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs',\n",
    "                             'numpy',\n",
    "                             'sklearn',\n",
    "                            # 'pickle5'\n",
    "                              ])\n",
    "def training(\n",
    "        X_TRAIN : Input[Dataset],\n",
    "        X_TEST : Input[Dataset],\n",
    "        y_TRAIN : Input[Dataset],\n",
    "        y_TEST : Input[Dataset],\n",
    "        MODEL: Output[Model]\n",
    "        #pipeline_metrics: Output[Metrics]) -> NamedTuple(\n",
    "        #  'ComponentOutputs',\n",
    "        #  [\n",
    "        #      ('xxxx', str),\n",
    "        #  ]\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    import sklearn as sk\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import preprocessing\n",
    "    import pickle\n",
    "\n",
    "    #Read Dataset Artifacts\n",
    "    X_train = pd.read_csv(X_TRAIN.path)\n",
    "    X_test = pd.read_csv(X_TEST.path)\n",
    "    y_train = pd.read_csv(y_TRAIN.path)\n",
    "    y_test = pd.read_csv(y_TEST.path)\n",
    "    \n",
    "    #Make Labels into numpy array\n",
    "    y_train = y_train.to_numpy().reshape((len(y_train)))\n",
    "    y_test = y_test.to_numpy().reshape((len(y_test)))\n",
    "    \n",
    "    #Initiate Scikit-Learn Training Process\n",
    "    LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=200).fit(X_train, y_train)\n",
    "    \n",
    "    # Export the classifier to a file\n",
    "    \n",
    "    #file_name = '/scikit_model.sav'\n",
    "    pickle.dump(LR, open(MODEL.path, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "reliable-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='scikitlearnclassifer01',\n",
    "  description='Binary Classification Model with Tensorflow Deep Learning and Connected Pre-processing Layers'\n",
    ")\n",
    "def scikit_classifier_earnings_v1(\n",
    "    pipeline: str = 'Scikit-Learn Earnings Classifer',\n",
    "    framework: str = 'Scikit-learn',\n",
    "    input_path: str = 'crazy-hippo-01.census_data_us.census_raw',\n",
    "    dataset_version: int = 3\n",
    "    ):\n",
    "    \n",
    "    first_step = pre_processing()\n",
    "   \n",
    "    second_step = training(first_step.outputs['X_TRAIN'],\n",
    "                          first_step.outputs['X_TEST'],\n",
    "                          first_step.outputs['y_TRAIN'],\n",
    "                          first_step.outputs['y_TEST']\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-authentication",
   "metadata": {},
   "source": [
    "### Compile and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "respected-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=scikit_classifier_earnings_v1, package_path=\"scikit_classifier_earnings_v1.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fresh-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "answering-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = 'pipelines-vertex-ai@crazy-hippo-01.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "disabled-investigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/scikitlearnclassifer01-20210625165125?project=crazy-hippo-01\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"scikit_classifier_earnings_v1.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    service_account=SERVICE_ACCOUNT \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-agent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-muscle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
