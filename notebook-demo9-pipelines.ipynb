{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Pipeline (Earnings Model) - AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubeflow workflow orchistration and Tensorflow model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import kfp\n",
    "from typing import NamedTuple\n",
    "from google.cloud import bigquery\n",
    "import logging\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_storage(raw_storage_input) -> NamedTuple('step_one_output', [('rows', float),\n",
    "                                                                           ('step_one_output_data', str)\n",
    "                                                                          ]):\n",
    "    #import libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    import pandas as pd\n",
    "    from collections import namedtuple\n",
    "    import gcsfs\n",
    "    \n",
    "    data = pd.read_csv(raw_storage_input)\n",
    "    \n",
    "    #Get number of row in dataset\n",
    "    rows_raw_data = len(data)\n",
    "    \n",
    "    #Write Staging data to bucket\n",
    "    write_to_storage = data.to_csv('gs://crazy-hippo-01/dataset/census_step_one.csv', index = False, header=True)\n",
    "    \n",
    "    step_one_output_data = 'gs://crazy-hippo-01/dataset/census_step_one.csv'\n",
    "\n",
    "    #Return number of rows\n",
    "    return(rows_raw_data, step_one_output_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_storage = kfp.components.func_to_container_op(read_from_storage, \n",
    "  output_component_file='./pipeline-components/step_1_read.component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(step_one_output_data, raw_rows) -> NamedTuple('step_two_output', [('clean_rows', int),\n",
    "                                                                                 ('data_preparation_data', str),\n",
    "                                                                                 ('data_validation_artifact', str)\n",
    "                                                                                ]):\n",
    "    #import libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    import pandas as pd\n",
    "    from collections import namedtuple\n",
    "    import gcsfs\n",
    "    \n",
    "    data = pd.read_csv(step_one_output_data)\n",
    "    \n",
    "    #Remove null values from workclass\n",
    "    null = data['workclass'].isna()\n",
    "    clean_data = data[-null] \n",
    "    rows_clean_data = int(len(clean_data))\n",
    "    \n",
    "    #Remove columns\n",
    "    final_df = clean_data[['age', 'workclass', 'gender', 'occupation', 'education_num', 'marital_status', \n",
    "                           'relationship', 'capital_gain', 'income_bracket']]\n",
    "    \n",
    "    #Write df to csv in storage bucket\n",
    "    write_to_storage = final_df.to_csv('gs://crazy-hippo-01/dataset/census_step_two.csv', index = False, header=True)\n",
    "    write_artifact_to_storage = final_df.to_csv('gs://crazy-hippo-01/dataset/artifact_validation.csv', index = False, header=False)\n",
    "    \n",
    "    data_preparation_data = 'gs://crazy-hippo-01/dataset/census_step_two.csv'\n",
    "    data_validation_artifact = 'gs://crazy-hippo-01/dataset/artifact_validation.csv'\n",
    "\n",
    "    #Return number of rows\n",
    "    return(rows_clean_data, data_preparation_data, data_validation_artifact)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = kfp.components.func_to_container_op(clean_data, \n",
    "  output_component_file='./pipeline-components/step_2_clean.component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(clean_rows, data_preparation_data, data_validation_artifact) -> NamedTuple('step_three_output', \n",
    "                                                                 [('train_rows', int), \n",
    "                                                                  ('test_rows', int), \n",
    "                                                                  ('columns_number', int),\n",
    "                                                                  ('x_train', str),\n",
    "                                                                  ('x_test', str),\n",
    "                                                                  ('y_train', str),\n",
    "                                                                  ('y_test', str),\n",
    "                                                                  ('x_val', str),\n",
    "                                                                  ('y_val', str)\n",
    "                                                                 ]):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    import pandas as pd\n",
    "    from collections import namedtuple\n",
    "    import gcsfs\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    #Import data\n",
    "    data = pd.read_csv(data_preparation_data)\n",
    "    \n",
    "    #Seperate X and y values\n",
    "    X = data[['age', 'workclass', 'gender', 'occupation', 'education_num', 'marital_status', 'relationship', 'capital_gain']]\n",
    "    y = data[['income_bracket']]\n",
    "    \n",
    "    #One-hot encode data\n",
    "    X = pd.get_dummies(X, prefix=['workclass', 'gender','occupation','marital_status','relationship'])\n",
    "    \n",
    "    #Normalize data\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X[['age','education_num','capital_gain']] = scaler.fit_transform(X[['age','education_num','capital_gain']])\n",
    "    \n",
    "    #Split data in train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    print(X_val.shape)\n",
    "    print(y_val.shape)\n",
    "\n",
    "    #Get row data from training and test data sets\n",
    "    train_rows = int(len(X_train))\n",
    "    test_rows = int(len(X_test))\n",
    "    columns_number = int(len(X_train.columns))\n",
    "    val_rows = int(len(X_val))\n",
    "    \n",
    "    #Save training and test datasets in bucket\n",
    "    x_train = X_train.to_csv(r'gs://crazy-hippo-01/dataset/x_train.csv', index = False, header=True)\n",
    "    x_test = X_test.to_csv(r'gs://crazy-hippo-01/dataset/x_test.csv', index = False, header=True)\n",
    "    y_train = y_train.to_csv(r'gs://crazy-hippo-01/dataset/y_train.csv', index = False, header=True)\n",
    "    y_test = y_test.to_csv(r'gs://crazy-hippo-01/dataset/y_test.csv', index = False, header=True)\n",
    "    x_val = X_val.to_csv(r'gs://crazy-hippo-01/dataset/x_val.csv', index = False, header=True)\n",
    "    y_val = y_val.to_csv(r'gs://crazy-hippo-01/dataset/y_val.csv', index = False, header=True)\n",
    "    \n",
    "    x_train = 'gs://crazy-hippo-01/dataset/x_train.csv'\n",
    "    x_test = 'gs://crazy-hippo-01/dataset/x_test.csv'\n",
    "    y_train = 'gs://crazy-hippo-01/dataset/y_train.csv'\n",
    "    y_test = 'gs://crazy-hippo-01/dataset/y_test.csv'\n",
    "    x_val = 'gs://crazy-hippo-01/dataset/x_val.csv'\n",
    "    y_val = 'gs://crazy-hippo-01/dataset/y_val.csv'\n",
    "    \n",
    "    return(train_rows, test_rows, columns_number, x_train, x_test, y_train, y_test, x_val, y_val)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation = kfp.components.func_to_container_op(data_preparation, \n",
    "  output_component_file='./pipeline-components/step_3_prep.component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(EPOCHS : int,\n",
    "                     BATCH_SIZE : int,\n",
    "                     VERBOSE : int,\n",
    "                     NB_CLASSES : int,\n",
    "                     N_HIDDEN :int,\n",
    "                     #VALIDATION_SPLIT : float,\n",
    "                     INPUT_SHAPE : int,\n",
    "                     x_train : str, \n",
    "                     x_test : str, \n",
    "                     y_train : str, \n",
    "                     y_test : str,\n",
    "                     x_val : str,\n",
    "                     y_val : str,\n",
    "                     train_rows : int, \n",
    "                     test_rows : int) -> NamedTuple('step_four_output', [('test_loss', float), \n",
    "                                                                          ('test_acc', float),\n",
    "                                                                          ('export_path', str),\n",
    "                                                                          ('mlpipeline_ui_metadata', 'UI_metadata')]):  \n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'fs-gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.0.0b0'])\n",
    "    #subprocess.run([sys.executable, '-q', 'pip', 'install', 'pyyaml', 'h5py'])\n",
    "    from collections import namedtuple\n",
    "    import gcsfs\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import preprocessing\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import datetime\n",
    "    import json\n",
    "    \n",
    "    print('imports done')\n",
    "    \n",
    "    print(tf.__version__)\n",
    "    \n",
    "    print('Loading data...')\n",
    "    X_train = pd.read_csv(x_train)\n",
    "    X_test = pd.read_csv(x_test)\n",
    "    y_train = pd.read_csv(y_train)\n",
    "    y_test = pd.read_csv(y_test)\n",
    "    x_val = pd.read_csv(x_val)\n",
    "    y_val = pd.read_csv(y_val)\n",
    "    print('Data uploaded')\n",
    "    \n",
    "    print(X_test.shape, y_test.shape)\n",
    "    \n",
    "    #Create a Tensorflow dataset\n",
    "    print('Creating tensorflow datasets')\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((x_val.values, y_val.values))\n",
    "    \n",
    "    training_dataset = training_dataset.shuffle(len(X_train)).batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.shuffle(len(x_val)).batch(BATCH_SIZE)\n",
    "    \n",
    "    #Tensorboard\n",
    "    log_dir = \"gs://crazy-hippo-01/dataset/logs/\"\n",
    "    \n",
    "    \n",
    "    #build the model\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(5,input_shape=(40,), activation='relu'),\n",
    "        #layers.Dense(10, activation='relu'),\n",
    "        layers.Dense(2, name='dense_layer_output', activation='sigmoid')\n",
    "      ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "                    #optimizer='SGD',\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    #loss='categorical_crossentropy',\n",
    "                    #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    #Train model\n",
    "    model.fit(training_dataset, \n",
    "              epochs= EPOCHS,\n",
    "              verbose=VERBOSE, \n",
    "              #steps_per_epoch=10,\n",
    "              #validation_steps=10, \n",
    "              validation_data=validation_dataset, \n",
    "              callbacks= [\n",
    "                  tf.keras.callbacks.TensorBoard(log_dir=log_dir + datetime.datetime.now().date().__str__()),\n",
    "                  tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss')\n",
    "              ]\n",
    ")\n",
    "    \n",
    "    #Metadata for Tensorboard\n",
    "    metadata = {'outputs' : [{\n",
    "        'type': 'tensorboard',\n",
    "        'source': log_dir,\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    #Save model in bucket    \n",
    "    #export_path = tf.contrib.saved_model.save_keras_model(\n",
    "    #    model, 'gs://crazy-hippo-01/dataset/census_tf_model')\n",
    "    #export_path = export_path.decode('utf-8')\n",
    "    \n",
    "    model.save('gs://crazy-hippo-01/dataset/census_tf_model')\n",
    "    \n",
    "    export_path = 'gs://crazy-hippo-01/dataset/census_tf_model'\n",
    "    \n",
    "    print('Model Saved.')\n",
    "    \n",
    "    \n",
    "    #Create evaluation metrics and save to variables\n",
    "    test_loss, test_acc = model.evaluate(test_dataset, steps=20)\n",
    "    \n",
    "    test_loss = float(test_loss)\n",
    "    test_acc = float(test_acc)\n",
    "    \n",
    "    return(test_loss, test_acc, export_path, metadata, json.dumps(metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training = kfp.components.func_to_container_op(model_training, \n",
    "  output_component_file='./pipeline-components/step_4_training.component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(test_loss, test_acc, export_path) -> NamedTuple('MyDivmodOutput', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    #import libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.0.0b0'])\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    #Creating Metrics\n",
    "    print(\"Creating JSON dump\")\n",
    "    metrics = {\n",
    "    'metrics': [\n",
    "        {\n",
    "          'name': 'loss-value', # The name of the metric. Visualized as the column name in the runs table.\n",
    "          'numberValue': test_loss, # The value of the metric. Must be a numeric value.\n",
    "          'format': \"RAW\"   # The optional format of the metric.* Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "        },\n",
    "        {\n",
    "          'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "          'numberValue': test_acc, # The value of the metric. Must be a numeric value.\n",
    "          'format': \"PERCENTAGE\" \n",
    "        }]\n",
    "    }\n",
    "    #Write JSON dump file\n",
    "    with file_io.FileIO('/mlpipeline-metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "        \n",
    "    print(\"JSON dump done\")\n",
    "    \n",
    "    \n",
    "    return(metrics, json.dumps(metrics))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation = kfp.components.func_to_container_op(model_evaluation, \n",
    "  output_component_file='./pipeline-components/step_5_evaluation.component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "  name='Classification Pipeline - Earnings Prediction',\n",
    "  description='Pipeline will read files from GCS, perform data cleaning and preparation and finally train and evaluate the model.'\n",
    ")\n",
    "def tensorflow_census(\n",
    "        #run = run_id,\n",
    "        raw_storage_input_data = 'gs://crazy-hippo-01/dataset/census_train.csv',\n",
    "        cleaning_input_data = 'gs://crazy-hippo-01/dataset/census_step_one.csv',\n",
    "        prepp_input_data = 'gs://crazy-hippo-01/dataset/census_step_two.csv',\n",
    "        EPOCHS : int = 20,\n",
    "        BATCH_SIZE : int = 32,\n",
    "        VERBOSE : int = 1,\n",
    "        NB_CLASSES : int = 2,\n",
    "        N_HIDDEN : int = 10,\n",
    "        VALIDATION_SPLIT : float = 0.2\n",
    "    ):\n",
    "        #Step 1:\n",
    "        step_one_read_from_storage = read_from_storage(raw_storage_input_data)\n",
    "        step_one_read_from_storage.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "        #Step 2:\n",
    "        step_two_cleaning_input = clean_data(step_one_read_from_storage.outputs['step_one_output_data'], \n",
    "                                                 step_one_read_from_storage.outputs['rows'] \n",
    "                                                 )\n",
    "        step_two_cleaning_input.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "        step_two_cleaning_input.after(step_one_read_from_storage)\n",
    "        \n",
    "        #Step 3:\n",
    "        step_three_data_preparation = data_preparation(step_two_cleaning_input.outputs['clean_rows'],\n",
    "                                                       step_two_cleaning_input.outputs['data_preparation_data'],\n",
    "                                                       step_two_cleaning_input.outputs['data_validation_artifact']\n",
    "                                                      )\n",
    "        step_three_data_preparation.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        step_three_data_preparation.after(step_two_cleaning_input)\n",
    "        \n",
    "        #Step 4:\n",
    "        step_four_model_training = model_training(\n",
    "                                         EPOCHS,\n",
    "                                         BATCH_SIZE,\n",
    "                                         VERBOSE,\n",
    "                                         NB_CLASSES,\n",
    "                                         N_HIDDEN,\n",
    "                                         #VALIDATION_SPLIT,\n",
    "                                         step_three_data_preparation.outputs['columns_number'],\n",
    "                                         step_three_data_preparation.outputs['x_train'],\n",
    "                                         step_three_data_preparation.outputs['x_test'],\n",
    "                                         step_three_data_preparation.outputs['y_train'],\n",
    "                                         step_three_data_preparation.outputs['y_test'],\n",
    "                                         step_three_data_preparation.outputs['x_val'],\n",
    "                                         step_three_data_preparation.outputs['y_val'],\n",
    "                                         step_three_data_preparation.outputs['train_rows'],\n",
    "                                         step_three_data_preparation.outputs['test_rows']  \n",
    "                                    )\n",
    "        step_four_model_training.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        step_four_model_training.after(step_three_data_preparation)\n",
    "        \n",
    "        \n",
    "        #Step 5:\n",
    "        step_five_model_evaluation = model_evaluation(step_four_model_training.outputs['test_loss'], \n",
    "                                                      step_four_model_training.outputs['test_acc'],\n",
    "                                                      step_four_model_training.outputs['export_path'])\n",
    "        step_five_model_evaluation.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "        step_five_model_evaluation.after(step_four_model_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Pipeline & Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(tensorflow_census,  \n",
    "  'earnings-pipeline-ver2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/472bfe1a-f79f-4316-ae3e-7f18817395b0\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/6ffe2acd-6b33-4b48-8c07-468e94e980a4\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = datetime.datetime.now()\n",
    "run_id = 'tensorflow_earnings.{}'.format(time.strftime(\"%Y %b %d %H:%M\"))\n",
    "#client = kfp.Client(host='6442b053d2d40df7-dot-us-east1.pipelines.googleusercontent.com')\n",
    "client = kfp.Client(host='18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com')\n",
    "classification_experiment = client.create_experiment(name='Earnings Prediction Pipeline', description='ML pipeline with Tensorflow')\n",
    "my_run = client.run_pipeline(classification_experiment.id, run_id, 'earnings-pipeline-ver2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
