{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c3048cd1427"
   },
   "source": [
    "# Vertex SDK: BigQuery Custom Container Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOMNWzTbftDr"
   },
   "source": [
    "# Install Vertex SDK\n",
    "\n",
    "\n",
    "After the SDK installation the kernel will be automatically restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be020jY-ftDv"
   },
   "outputs": [],
   "source": [
    "!pip3 uninstall -y google-cloud-aiplatform\n",
    "!pip3 install google-cloud-aiplatform\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181b681faf5c"
   },
   "source": [
    "### Enter your project and GCS bucket\n",
    "\n",
    "Enter your Project Id in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d61oYG3KftDw"
   },
   "outputs": [],
   "source": [
    "MY_PROJECT = \"crazy-hippo-01\"\n",
    "MY_STAGING_BUCKET = \"gs://crazy-vertex-ai-pipelines\"  # bucket should be in same region as ucaip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T1d5uBoftDw"
   },
   "source": [
    "# Copy Big Query Iris Dataset\n",
    "We will make a Big Query dataset and copy Big Query's public iris table to that dataset. For more information about this dataset please visit: https://archive.ics.uci.edu/ml/datasets/iris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJF047yNftDw"
   },
   "source": [
    "### Make BQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9yOl-l_oftDx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'crazy-hippo-01:ml_iris' successfully created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MY_PROJECT'] = MY_PROJECT\n",
    "!bq mk {MY_PROJECT}:ml_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn9TuBZAftDx"
   },
   "source": [
    "### Copy bigquery-public-data.ml_datasets.iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ISFR8nFfftDx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r72dbc8e433bbd436_00000179a2c5512d_1 ... (0s) Current status: DONE   \n",
      "Table 'bigquery-public-data:ml_datasets.iris' successfully copied to 'crazy-hippo-01:ml_iris.iris_raw'\n"
     ]
    }
   ],
   "source": [
    "!bq cp -n bigquery-public-data:ml_datasets.iris {MY_PROJECT}:ml_iris.iris_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IltwFqKIftDx"
   },
   "source": [
    "# Create Training Container\n",
    "We will create a directory and write all of our container build artifacts into that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "40BkhtMeftDy"
   },
   "outputs": [],
   "source": [
    "CONTAINER_ARTIFACTS_DIR = \"demo-container-artifacts\"\n",
    "\n",
    "!mkdir {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVeG-LPOftDy"
   },
   "source": [
    "### Create Cloudbuild YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4kODuFZCftDy"
   },
   "outputs": [],
   "source": [
    "cloudbuild_yaml = \"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/{MY_PROJECT}/test-custom-container', '.' ]\n",
    "images: ['gcr.io/{MY_PROJECT}/test-custom-container']\"\"\".format(\n",
    "    MY_PROJECT=MY_PROJECT\n",
    ")\n",
    "\n",
    "with open(f\"{CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ_GUCtZftDz"
   },
   "source": [
    "### Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rja_jo3rftDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo-container-artifacts/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/Dockerfile\n",
    "\n",
    "# Specifies base image and tag\n",
    "FROM gcr.io/google-appengine/python\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip3 install tensorflow tensorflow-io pyarrow\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY test_script.py /root/test_script.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"test_script.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dfrLShaftDz"
   },
   "source": [
    "### Write entrypoint script to invoke trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S0jSd8NWftDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo-container-artifacts/test_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/test_script.py\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import os\n",
    "\n",
    "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
    "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "data_format = os.environ[\"AIP_DATA_FORMAT\"]\n",
    "\n",
    "def caip_uri_to_fields(uri):\n",
    "    uri = uri[5:]\n",
    "    project, dataset, table = uri.split('.')\n",
    "    return project, dataset, table\n",
    "\n",
    "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "target_name = 'species'\n",
    "\n",
    "def transform_row(row_dict):\n",
    "  # Trim all string tensors\n",
    "  trimmed_dict = { column:\n",
    "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                  for (column,tensor) in row_dict.items()\n",
    "                  }\n",
    "  target = trimmed_dict.pop(target_name)\n",
    "\n",
    "  target_float = tf.cond(tf.equal(tf.strings.strip(target), 'versicolor'), \n",
    "                 lambda: tf.constant(1.0),\n",
    "                 lambda: tf.constant(0.0))\n",
    "  return (trimmed_dict, target_float)\n",
    "\n",
    "def read_bigquery(project, dataset, table):\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + project,\n",
    "      project, table, dataset,\n",
    "      feature_names + [target_name],\n",
    "      [dtypes.float64] * 4 + [dtypes.string],\n",
    "      requested_streams=2)\n",
    "\n",
    "  dataset = read_session.parallel_read_rows()\n",
    "  transformed_ds = dataset.map(transform_row)\n",
    "  return transformed_ds\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "training_ds = read_bigquery(*caip_uri_to_fields(training_data_uri)).shuffle(10).batch(BATCH_SIZE)\n",
    "eval_ds = read_bigquery(*caip_uri_to_fields(validation_data_uri)).batch(BATCH_SIZE)\n",
    "test_ds = read_bigquery(*caip_uri_to_fields(test_data_uri)).batch(BATCH_SIZE)\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in feature_names:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "model = tf.keras.Sequential(\n",
    "  [\n",
    "    feature_layer,\n",
    "      Dense(16, activation=tf.nn.relu),\n",
    "      Dense(8, activation=tf.nn.relu),\n",
    "      Dense(4, activation=tf.nn.relu),\n",
    "      Dense(1, activation=tf.nn.sigmoid),\n",
    "  ])\n",
    "\n",
    "# Compile Keras model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "    optimizer='adam')\n",
    "\n",
    "model.fit(training_ds, epochs=5, validation_data=eval_ds)\n",
    "\n",
    "print(model.evaluate(test_ds))\n",
    "\n",
    "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LYlV4D2ftD0"
   },
   "source": [
    "### Build Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9tGdX7B_ftD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 3.1 KiB before compression.\n",
      "Uploading tarball of [demo-container-artifacts] to [gs://crazy-hippo-01_cloudbuild/source/1621933595.237262-0cb10e9fe2dc4248bb61c8cb3030a612.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/crazy-hippo-01/locations/global/builds/a373a1e5-cb09-4224-9304-d2f2703449ab].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/a373a1e5-cb09-4224-9304-d2f2703449ab?project=433654631026].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"a373a1e5-cb09-4224-9304-d2f2703449ab\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://crazy-hippo-01_cloudbuild/source/1621933595.237262-0cb10e9fe2dc4248bb61c8cb3030a612.tgz#1621934315511773\n",
      "Copying gs://crazy-hippo-01_cloudbuild/source/1621933595.237262-0cb10e9fe2dc4248bb61c8cb3030a612.tgz#1621934315511773...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/5 : FROM gcr.io/google-appengine/python\n",
      "latest: Pulling from google-appengine/python\n",
      "Digest: sha256:fc9d9f429bf75b8369f7894b7eb24098418050557b889f4b02aa4c93fdb5e161\n",
      "Status: Downloaded newer image for gcr.io/google-appengine/python:latest\n",
      " ---> 96b4d9af6594\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Running in a26a1665477e\n",
      "Removing intermediate container a26a1665477e\n",
      " ---> f2a09710a930\n",
      "Step 3/5 : RUN pip3 install tensorflow tensorflow-io pyarrow\n",
      " ---> Running in 567d23486bae\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "Collecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-4.0.0-cp37-cp37m-manylinux2014_x86_64.whl (21.8 MB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.18.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-57.0.0-py3-none-any.whl (821 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting cached-property; python_version < \"3.8\"\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4001 sha256=ea9831863a33338ccdbda03741c193ae3cc6148010fa55a1cca90eba6cad8baa\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=65542 sha256=ed9ed6fc479564ec666c7ce9946dff6301a0ab3fd2ee56c2d9d0cda5dfb2ae5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: six, grpcio, typing-extensions, keras-nightly, tensorflow-estimator, flatbuffers, termcolor, gast, wheel, astunparse, protobuf, numpy, keras-preprocessing, setuptools, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, chardet, idna, urllib3, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, absl-py, zipp, importlib-metadata, markdown, tensorboard-data-server, werkzeug, tensorboard, opt-einsum, google-pasta, wrapt, cached-property, h5py, tensorflow, tensorflow-io-gcs-filesystem, tensorflow-io, pyarrow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.31.1\n",
      "    Uninstalling wheel-0.31.1:\n",
      "      Successfully uninstalled wheel-0.31.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.2.0\n",
      "    Uninstalling setuptools-40.2.0:\n",
      "      Successfully uninstalled setuptools-40.2.0\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.2 certifi-2020.12.5 chardet-4.0.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.30.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 idna-2.10 importlib-metadata-4.0.1 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.17.1 pyarrow-4.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-57.0.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 tensorflow-io-0.18.0 tensorflow-io-gcs-filesystem-0.18.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.4 werkzeug-2.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n",
      "\u001b[91mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 567d23486bae\n",
      " ---> f098e2c3fc3f\n",
      "Step 4/5 : COPY test_script.py /root/test_script.py\n",
      " ---> 22a6dd811a72\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"test_script.py\"]\n",
      " ---> Running in 11da42c216e9\n",
      "Removing intermediate container 11da42c216e9\n",
      " ---> d537f5e2c919\n",
      "Successfully built d537f5e2c919\n",
      "Successfully tagged gcr.io/crazy-hippo-01/test-custom-container:latest\n",
      "PUSH\n",
      "Pushing gcr.io/crazy-hippo-01/test-custom-container\n",
      "The push refers to repository [gcr.io/crazy-hippo-01/test-custom-container]\n",
      "299b28be9f0c: Preparing\n",
      "78c44f8aed97: Preparing\n",
      "3ec03be1bb38: Preparing\n",
      "c5f91450ff31: Preparing\n",
      "d8a99d217f22: Preparing\n",
      "88cace8eb32b: Preparing\n",
      "ae3630d8a5da: Preparing\n",
      "f4abe2e82c92: Preparing\n",
      "a43e6a3baa91: Preparing\n",
      "41e2d890e349: Preparing\n",
      "84ff92691f90: Preparing\n",
      "6071362d21ac: Preparing\n",
      "965eb7efdb65: Preparing\n",
      "a43e6a3baa91: Waiting\n",
      "41e2d890e349: Waiting\n",
      "84ff92691f90: Waiting\n",
      "6071362d21ac: Waiting\n",
      "965eb7efdb65: Waiting\n",
      "88cace8eb32b: Waiting\n",
      "ae3630d8a5da: Waiting\n",
      "f4abe2e82c92: Waiting\n",
      "3ec03be1bb38: Pushed\n",
      "299b28be9f0c: Pushed\n",
      "c5f91450ff31: Pushed\n",
      "88cace8eb32b: Pushed\n",
      "a43e6a3baa91: Pushed\n",
      "41e2d890e349: Pushed\n",
      "84ff92691f90: Layer already exists\n",
      "6071362d21ac: Pushed\n",
      "d8a99d217f22: Pushed\n",
      "965eb7efdb65: Pushed\n",
      "ae3630d8a5da: Pushed\n",
      "f4abe2e82c92: Pushed\n",
      "78c44f8aed97: Pushed\n",
      "latest: digest: sha256:1275c95f6c4c013d0fe72f932b49b2e5d5c5fc870555c99fd9afa461cd7327ad size: 3043\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                 STATUS\n",
      "a373a1e5-cb09-4224-9304-d2f2703449ab  2021-05-25T09:18:35+00:00  4M31S     gs://crazy-hippo-01_cloudbuild/source/1621933595.237262-0cb10e9fe2dc4248bb61c8cb3030a612.tgz  gcr.io/crazy-hippo-01/test-custom-container (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config {CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf0pugbvftD1"
   },
   "source": [
    "# Run Custom Container Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ee691569d8d"
   },
   "source": [
    "## Initialize Vertex AI SDK\n",
    "\n",
    "Initialize the *client* for Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vEEr62NUftD1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "MY_PROJECT = \"crazy-hippo-01\"\n",
    "\n",
    "aiplatform.init(project=MY_PROJECT, staging_bucket=MY_STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "736ddff8408b"
   },
   "source": [
    "# Create a Managed Tabular Dataset from Big Query Dataset\n",
    "\n",
    "This section will create a managed Tabular dataset from the iris Big Query table we copied above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oBdOv6lWftD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/433654631026/locations/us-central1/datasets/7915155509940846592/operations/2700835414666641408\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/433654631026/locations/us-central1/datasets/7915155509940846592\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/433654631026/locations/us-central1/datasets/7915155509940846592')\n"
     ]
    }
   ],
   "source": [
    "ds = aiplatform.TabularDataset.create(\n",
    "    display_name=\"bq_iris_dataset\", bq_source=f\"bq://{MY_PROJECT}.ml_iris.iris_raw\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee242cc1f74c"
   },
   "source": [
    "# Launch a Training Job to Create a Model\n",
    "\n",
    "We will train a model with the container we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uRGrFdxOftD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://crazy-vertex-ai-pipelines/aiplatform-custom-training-2021-05-25-09:15:06.583 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7244312679509131264?project=433654631026\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/433654631026/locations/us-central1/trainingPipelines/7244312679509131264\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/433654631026/locations/us-central1/models/2951529013601894400\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"train-bq-iris\",\n",
    "    container_uri=f\"gcr.io/{MY_PROJECT}/test-custom-container:latest\",\n",
    "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\",\n",
    ")\n",
    "model = job.run(\n",
    "    ds,\n",
    "    replica_count=1,\n",
    "    model_display_name=\"bq-iris-model\",\n",
    "    bigquery_destination=f\"bq://{MY_PROJECT}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7fa9b59f919"
   },
   "source": [
    "# Deploy Your Model\n",
    "\n",
    "Deploy your model, then wait until the model FINISHES deployment before proceeding to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tEg2IDwPftD2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/433654631026/locations/us-central1/endpoints/8919431838565400576/operations/5156423111490404352\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/433654631026/locations/us-central1/endpoints/8919431838565400576\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/433654631026/locations/us-central1/endpoints/8919431838565400576')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/433654631026/locations/us-central1/endpoints/8919431838565400576\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/433654631026/locations/us-central1/endpoints/8919431838565400576/operations/3805343223279255552\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/433654631026/locations/us-central1/endpoints/8919431838565400576\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.models.Endpoint object at 0x7fe9769cde10> \n",
       " resource name: projects/433654631026/locations/us-central1/endpoints/8919431838565400576,\n",
       " <google.cloud.aiplatform.models.Endpoint object at 0x7fe9769dde10> \n",
       " resource name: projects/433654631026/locations/us-central1/endpoints/6696113774086062080,\n",
       " <google.cloud.aiplatform.models.Endpoint object at 0x7fe977251c50> \n",
       " resource name: projects/433654631026/locations/us-central1/endpoints/2987645771551080448]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.Endpoint.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dbd6c650a03"
   },
   "source": [
    "# Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction is a binary classification for <b>if the flower is a versicolor iris flower of not</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DKVhGB1PftD2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.334066838]], deployed_model_id='4748957846131965952', explanations=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(\n",
    "    [{\"sepal_length\": 4.1, \"sepal_width\": 2.5, \"petal_length\": 1.0, \"petal_width\": 1.1}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Platform_(Unified)_SDK_BigQuery_Custom_Container_Training.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
