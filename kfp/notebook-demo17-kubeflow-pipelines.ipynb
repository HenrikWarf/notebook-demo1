{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lonely-robert",
   "metadata": {},
   "source": [
    "# Kubeflow Pipelines Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-softball",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "buried-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kfp\n",
    "import seaborn as sb\n",
    "from kfp import compiler\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-product",
   "metadata": {},
   "source": [
    "### Kubeflow Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "victorian-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='https://18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-memorial",
   "metadata": {},
   "source": [
    "### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "photographic-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'gcs://crazy-hippo-01/kubeflow_staging/'\n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-offense",
   "metadata": {},
   "source": [
    "### Pipeline Component - READ & TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "changed-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def read_transform_data() -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "        ('training_data', str),\n",
    "        ('test_data', str),\n",
    "        ('validation_data', str),\n",
    "        #('product', float),\n",
    "        #('mlpipeline_ui_metadata', 'UI_metadata'),\n",
    "        #('mlpipeline_metrics', 'Metrics')\n",
    "      ]):\n",
    "    \n",
    "    #Import libraries\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client()\n",
    "    \n",
    "    query = \"\"\"SELECT age, workclass, occupation, education_num, marital_status, capital_gain, income_bracket\n",
    "    FROM `crazy-hippo-01.census_data_us.census_raw` \n",
    "    \"\"\"\n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    df = job.to_dataframe()\n",
    "    \n",
    "    #Drop null values in dataset\n",
    "    df = df.dropna()\n",
    "    \n",
    "    #Create training, test and validation datasets\n",
    "    train, test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "    train, val = train_test_split(train, test_size=0.20, random_state=42)\n",
    "\n",
    "    #Define Staging Bucket in GCS\n",
    "    BUCKET = 'gcs://crazy-hippo-01/kubeflow_staging/'\n",
    "    \n",
    "    #Define Datasets Names\n",
    "    TRAIN_DATA = BUCKET + 'datasets/training/training{}'.format(str(int(time.time())))  + '.csv'\n",
    "    TEST_DATA = BUCKET + 'datasets/testing/test{}'.format(str(int(time.time())))  + '.csv'\n",
    "    VALIDATION_DATA = BUCKET + 'datasets/validation/validation{}'.format(str(int(time.time())))  + '.csv'\n",
    "\n",
    "    #Write data to GCS Storage\n",
    "    train.to_csv(TRAIN_DATA, index=False, header=True)\n",
    "    test.to_csv(TEST_DATA, index=False, header=True)\n",
    "    val.to_csv(VALIDATION_DATA, index=False, header=True)\n",
    "\n",
    "    #Define outputs with namedtuple\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    return_values = namedtuple(\n",
    "      'ComponentOutputs',\n",
    "        ['training_data', 'test_data', 'validation_data'])\n",
    "        \n",
    "    return return_values(TRAIN_DATA, TEST_DATA, VALIDATION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "enabling-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_transform_comp = comp.create_component_from_func(\n",
    "    read_transform_data, \n",
    "    base_image='python:3.7',\n",
    "    output_component_file='read_transform_data.yaml',\n",
    "    packages_to_install=['pandas==0.24', \n",
    "                         'google-cloud-bigquery', \n",
    "                         'pyarrow' , \n",
    "                         'gcsfs', \n",
    "                         'sklearn']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-twenty",
   "metadata": {},
   "source": [
    "### Pipeline Component - PREPROCESS and TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "restricted-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(TRAIN_DATA, TEST_DATA, VALIDATION_DATA) -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "        ('model_path', str)\n",
    "      ]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental import preprocessing\n",
    "    \n",
    "    #VARIABLES AND TRAINING PARAMETERS\n",
    "    TRAINING_DATA = pd.read_csv(TRAIN_DATA)\n",
    "    TESTING_DATA = pd.read_csv(TEST_DATA)\n",
    "    VALIDATION_DATA = pd.read_csv(VALIDATION_DATA)\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    print(tf.__version__)\n",
    "    \n",
    "    print(TRAINING_DATA.head())\n",
    "    \n",
    "    print(TESTING_DATA.head())\n",
    "    \n",
    "    print(VALIDATION_DATA.head())\n",
    "    \n",
    "    #TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('income_bracket')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #NORMALIZATION FUNCTION\n",
    "    def helperfunc_get_normalization_layer(name, dataset):\n",
    "        # Create a Normalization layer for our feature.\n",
    "        normalizer = preprocessing.Normalization()\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the statistics of the data.\n",
    "        normalizer.adapt(feature_ds)\n",
    "\n",
    "        return normalizer\n",
    "    \n",
    "    #CATEGORY ENCODING FUNCTION\n",
    "    def helperfunc_get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "        # Create a StringLookup layer which will turn strings into integer indices\n",
    "        if dtype == 'string':\n",
    "            index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "        else:\n",
    "            index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the set of possible values and assign them a fixed integer index.\n",
    "        index.adapt(feature_ds)\n",
    "\n",
    "        # Create a Discretization for our integer indices.\n",
    "        encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = feature_ds.map(index)\n",
    "\n",
    "        # Learn the space of possible indices.\n",
    "        encoder.adapt(feature_ds)\n",
    "\n",
    "        # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "        # layer so we can use them, or include them in the functional model later.\n",
    "        return lambda feature: encoder(index(feature))\n",
    "    \n",
    "    #CREATE TENSORFLOW DATASETS\n",
    "    TRAIN_DS = helperfunc_create_dataset(TRAINING_DATA, batch_size=BATCH_SIZE)\n",
    "    VALIDATION_DS = helperfunc_create_dataset(VALIDATION_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #CREATE PREPROCESSING LAYERS\n",
    "    ALL_INPUTS = []\n",
    "    ENCODED_FEATURES = []\n",
    "\n",
    "    NUMERICAL = ['age' , 'capital_gain']\n",
    "    CATEGORICAL_INT_COLS = ['education_num']\n",
    "    CATEGORICAL_STRING_COLS = ['occupation', 'workclass', 'marital_status']\n",
    "    TARGET = ['income_bracket']\n",
    "    \n",
    "    # Numeric features.\n",
    "    for header in NUMERICAL:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = helperfunc_get_normalization_layer(header, TRAIN_DS)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        ALL_INPUTS.append(numeric_col)\n",
    "        ENCODED_FEATURES.append(encoded_numeric_col)\n",
    "        \n",
    "    # Categorical features encoded as integers.\n",
    "    for header in CATEGORICAL_INT_COLS:\n",
    "        categorical_int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='int64', max_tokens=5)\n",
    "        encoded_categorical_int_col = encoding_layer(categorical_int_col)\n",
    "        ALL_INPUTS.append(categorical_int_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_int_col)\n",
    "    \n",
    "    # Categorical features encoded as string.\n",
    "    for header in CATEGORICAL_STRING_COLS:\n",
    "        categorical_string_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='string', max_tokens=5)\n",
    "        encoded_categorical_string_col = encoding_layer(categorical_string_col)\n",
    "        ALL_INPUTS.append(categorical_string_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_string_col)\n",
    "    \n",
    "        \n",
    "    #CREATE and COMPILE MODEL\n",
    "    all_features = tf.keras.layers.concatenate(ENCODED_FEATURES)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(ALL_INPUTS, output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    #TRAIN MODEL\n",
    "    history = model.fit(TRAIN_DS, epochs=10, validation_data=VALIDATION_DS)\n",
    "    \n",
    "    \n",
    "    #Define Bucket in GCS for Model Storage\n",
    "    BUCKET = 'gs://crazy-hippo-01/kubeflow_staging/models/'\n",
    "    \n",
    "    #Define Datasets Names\n",
    "    MODEL_PATH = BUCKET + 'earnings_model{}'.format(str(int(time.time())))\n",
    "    \n",
    "    #Save model to Artifact Store for Project\n",
    "    model.save(MODEL_PATH)\n",
    "    \n",
    "    print('Model saved to: ' + MODEL_PATH)\n",
    "    \n",
    "    #Define outputs with namedtuple\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    return_values = namedtuple(\n",
    "      'ComponentOutputs',\n",
    "        ['model_path'])\n",
    "        \n",
    "    return return_values(MODEL_PATH)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "chief-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comp = comp.create_component_from_func(\n",
    "    train_model, \n",
    "    base_image='python:3.7',\n",
    "    output_component_file='train_model.yaml',\n",
    "    packages_to_install=['pandas==0.24', \n",
    "                         'pyarrow' , \n",
    "                         'gcsfs' , \n",
    "                         'google-cloud-bigquery-storage',\n",
    "                         'tensorflow==2.3.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-access",
   "metadata": {},
   "source": [
    "### Evaluate and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "positive-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validate_model(saved_model, test_dataset, pipeline, framework) -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "        ('mlpipeline_metrics', 'Metrics')  \n",
    "      ]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    \n",
    "    print(saved_model, test_dataset, pipeline, framework)\n",
    "    \n",
    "    #HELPER FUNCTION - TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('income_bracket')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #LOAD TRAINED MODEL FROM ARTIFACT STORE\n",
    "    reloaded_model = tf.keras.models.load_model(saved_model)\n",
    "    \n",
    "    #READ TESTING DATASET\n",
    "    TESTING_DATA = pd.read_csv(test_dataset)\n",
    "\n",
    "    #SET BATCG SIZE\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    #CALL HELPER FUNCTION TO CREATE TENSORFLOW DATASET\n",
    "    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #EVALUATE MODEL WITH TEST DATA\n",
    "    loss, accuracy = reloaded_model.evaluate(TESTING_DS)\n",
    "    \n",
    "    #PRINT ACCURACY METRIC\n",
    "    print(\"Accuracy\", accuracy)\n",
    "    print(\"Loss\", loss)\n",
    "    \n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import json\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'accuracy',\n",
    "          'numberValue':  accuracy,\n",
    "          'format': \"PERCENTAGE\",\n",
    "        },{\n",
    "          'name': 'loss',\n",
    "          'numberValue':  float(loss),\n",
    "    }]}\n",
    "    \n",
    "    #Write Metrics to BigQuery Table for Validation and possible promotion to Deployment\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client()\n",
    "    \n",
    "    #Define DML Query to Insert Metrics into BugQuery\n",
    "    query = \"\"\"INSERT `crazy-hippo-01.census_data_us.model_metrics_history` (model_name, pipeline, framework, accuracy, loss)\n",
    "    VALUES (\"{}\", \"{}\", \"{}\", {}, {})  \n",
    "    \"\"\".format(saved_model, pipeline, framework, accuracy, loss)\n",
    "    \n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    \n",
    "    \n",
    "    #Define outputs with namedtuple\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    return_values = namedtuple(\n",
    "      'ComponentOutputs',\n",
    "        ['mlpipeline_metrics'])\n",
    "        \n",
    "    return return_values(json.dumps(metrics))\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "religious-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_validate_comp = comp.create_component_from_func(\n",
    "    evaluate_validate_model, \n",
    "    base_image='python:3.7',\n",
    "    output_component_file='evaluate_model.yaml',\n",
    "    packages_to_install=['pandas==0.24',\n",
    "                         'google-cloud-bigquery',\n",
    "                         'pyarrow', \n",
    "                         'gcsfs',\n",
    "                         'tensorflow']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-fountain",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "departmental-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Tensorflow DL model with Integrated Preprocessing (Version 1)',\n",
    "  description='Binary Classification Model with Tensorflow Deep Learning and Connected Pre-processing Layers'\n",
    ")\n",
    "def binary_classifier_earnings(\n",
    "    pipeline = 'Tensorflow DL model with Integrated Preprocessing (Version 1)',\n",
    "    framework = 'Tensorflow'\n",
    "    ):\n",
    "   \n",
    "    first_step = read_transform_comp()\n",
    "    first_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "   \n",
    "    second_step = train_comp(first_step.outputs['training_data'], \n",
    "                             first_step.outputs['test_data'], \n",
    "                             first_step.outputs['validation_data'])\n",
    "    second_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    \n",
    "    third_step = evaluate_validate_comp(second_step.outputs['model_path'], \n",
    "                                        first_step.outputs['test_data'],\n",
    "                                       pipeline, framework)\n",
    "    third_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-invalid",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "special-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-queue",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "statewide-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/e51b154e-47de-49c6-aa3f-51c4331825f5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/ec1b4a70-cf21-4143-b38e-4bb9429c3146\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=ec1b4a70-cf21-4143-b38e-4bb9429c3146)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(binary_classifier_earnings, arguments=arguments, experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-understanding",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "competent-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Earnings per Year - Binary Classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "conservative-school",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-28 11:05:56:INFO:Creating experiment Earnings per Year - Binary Classifier.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/e51b154e-47de-49c6-aa3f-51c4331825f5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get or create an experiment\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-annual",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
