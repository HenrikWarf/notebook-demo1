{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attended-mexican",
   "metadata": {},
   "source": [
    "# Productionazing Machine Learning with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-transsexual",
   "metadata": {},
   "source": [
    "### Set up project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "offshore-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  crazy-hippo-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"crazy-hippo-01\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-pointer",
   "metadata": {},
   "source": [
    "### Define Current Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "spectacular-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp:  20210525122223\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print('Timestamp: ', TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-enough",
   "metadata": {},
   "source": [
    "### Create storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "tough-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://crazy-vertex-ai-pipelines\"  # @param {type:\"string\"} \n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "opposite-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"-vertex-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "elementary-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://crazy-vertex-ai-pipelines/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spare-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-vegetable",
   "metadata": {},
   "source": [
    "### Import Libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "precious-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://crazy-vertex-ai-pipelines/pipeline_root/crazy-hippo'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "USER = \"crazy-hippo\"  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "sought-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    InputPath\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-western",
   "metadata": {},
   "source": [
    "### Define Pipeline Comnponents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-accent",
   "metadata": {},
   "source": [
    "#### Pipeline Step 1 - Extract and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "false-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='read_transform_data.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'google-cloud-bigquery', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs', \n",
    "                             'sklearn'])\n",
    "def read_transform_data(\n",
    "        INPUT_DATA : str,\n",
    "        TRAINING_DATA : Output[Dataset],\n",
    "        TEST_DATA : Output[Dataset],\n",
    "        VALIDATION_DATA : Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    #Import libraries\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #INPUT_DATA.uri = 'crazy-hippo-01.census_data_us.census_raw'\n",
    "    #INPUT_DATA.name = 'Earnings RAW Data Version 1'\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client(project='crazy-hippo-01')\n",
    "    \n",
    "    query = \"\"\"SELECT age, workclass, occupation, education_num, marital_status, capital_gain, income_bracket\n",
    "    FROM `crazy-hippo-01.census_data_us.census_raw` \n",
    "    \"\"\"\n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    #retry=bq_retry.DEFAULT_RETRY\n",
    "    df = job.to_dataframe()\n",
    "    \n",
    "    #Drop null values in dataset\n",
    "    df = df.dropna()\n",
    "    \n",
    "    #Create training, test and validation datasets\n",
    "    train, test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "    train, val = train_test_split(train, test_size=0.20, random_state=42)\n",
    "\n",
    "    #Define Staging Bucket in GCS\n",
    "    #BUCKET = 'gcs://crazy-hippo-01/kubeflow_staging/earnings_model/'\n",
    "    #TRAINING_PATH = BUCKET + 'datasets/training/training{}'.format(str(int(time.time())))  + '.csv'\n",
    "    #TEST_PATH = BUCKET + 'datasets/testing/test{}'.format(str(int(time.time())))  + '.csv'\n",
    "    #VALIDATION_PATH = BUCKET + 'datasets/validation/validation{}'.format(str(int(time.time())))  + '.csv'\n",
    "    \n",
    "    #Define Datasets Names\n",
    "    #TRAINING_DATA.uri = TRAINING_PATH\n",
    "    #TEST_DATA.uri = TEST_PATH\n",
    "    #VALIDATION_DATA.uri = VALIDATION_PATH\n",
    "    \n",
    "    print(TRAINING_DATA.path)\n",
    "    print(TEST_DATA.path)\n",
    "    print(VALIDATION_DATA.path)\n",
    "\n",
    "    #Write data to GCS Storage\n",
    "    train.to_csv(TRAINING_DATA.path, index=False, header=True)\n",
    "    test.to_csv(TEST_DATA.path, index=False, header=True)\n",
    "    val.to_csv(VALIDATION_DATA.path, index=False, header=True)\n",
    "\n",
    "    #Define outputs with namedtuple\n",
    "    #from collections import namedtuple\n",
    "    \n",
    "    #return_values = namedtuple(\n",
    "    #  'ComponentOutputs',\n",
    "    #    ['training_data', 'test_data', 'validation_data'])\n",
    "        \n",
    "    #return TRAINING_DATA, TEST_DATA), VALIDATION_DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-campus",
   "metadata": {},
   "source": [
    "#### Pipeline Step 2 - Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "earlier-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='train_model.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs' , \n",
    "                             'google-cloud-bigquery-storage',\n",
    "                             'tensorflow'])\n",
    "def train_model(TRAINING_DATA: Input[Dataset], \n",
    "                TEST_DATA: Input[Dataset], \n",
    "                VALIDATION_DATA: Input[Dataset],\n",
    "                MODEL: Output[Model]\n",
    "               ):\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental import preprocessing\n",
    "    \n",
    "    #VARIABLES AND TRAINING PARAMETERS\n",
    "    TRAIN_DATA = pd.read_csv(TRAINING_DATA.path)\n",
    "    TEST_DATA = pd.read_csv(TEST_DATA.path)\n",
    "    VAL_DATA = pd.read_csv(VALIDATION_DATA.path)\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    print(tf.__version__)\n",
    "    \n",
    "    print(MODEL.path)\n",
    "\n",
    "    #TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('income_bracket')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #NORMALIZATION FUNCTION\n",
    "    def helperfunc_get_normalization_layer(name, dataset):\n",
    "        # Create a Normalization layer for our feature.\n",
    "        normalizer = preprocessing.Normalization()\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the statistics of the data.\n",
    "        normalizer.adapt(feature_ds)\n",
    "\n",
    "        return normalizer\n",
    "    \n",
    "    #CATEGORY ENCODING FUNCTION\n",
    "    def helperfunc_get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "        # Create a StringLookup layer which will turn strings into integer indices\n",
    "        if dtype == 'string':\n",
    "            index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "        else:\n",
    "            index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the set of possible values and assign them a fixed integer index.\n",
    "        index.adapt(feature_ds)\n",
    "\n",
    "        # Create a Discretization for our integer indices.\n",
    "        encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = feature_ds.map(index)\n",
    "\n",
    "        # Learn the space of possible indices.\n",
    "        encoder.adapt(feature_ds)\n",
    "\n",
    "        # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "        # layer so we can use them, or include them in the functional model later.\n",
    "        return lambda feature: encoder(index(feature))\n",
    "    \n",
    "    #CREATE TENSORFLOW DATASETS\n",
    "    TRAIN_DS = helperfunc_create_dataset(TRAIN_DATA, batch_size=BATCH_SIZE)\n",
    "    VALIDATION_DS = helperfunc_create_dataset(VAL_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    TESTING_DS = helperfunc_create_dataset(TEST_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #CREATE PREPROCESSING LAYERS\n",
    "    ALL_INPUTS = []\n",
    "    ENCODED_FEATURES = []\n",
    "\n",
    "    NUMERICAL = ['age' , 'capital_gain']\n",
    "    CATEGORICAL_INT_COLS = ['education_num']\n",
    "    CATEGORICAL_STRING_COLS = ['occupation', \n",
    "                               'workclass', \n",
    "                               'marital_status']\n",
    "    TARGET = ['income_bracket']\n",
    "    \n",
    "    # Numeric features.\n",
    "    for header in NUMERICAL:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = helperfunc_get_normalization_layer(header, TRAIN_DS)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        ALL_INPUTS.append(numeric_col)\n",
    "        ENCODED_FEATURES.append(encoded_numeric_col)\n",
    "        \n",
    "    # Categorical features encoded as integers.\n",
    "    for header in CATEGORICAL_INT_COLS:\n",
    "        categorical_int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='int64', max_tokens=5)\n",
    "        encoded_categorical_int_col = encoding_layer(categorical_int_col)\n",
    "        ALL_INPUTS.append(categorical_int_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_int_col)\n",
    "    \n",
    "    # Categorical features encoded as string.\n",
    "    for header in CATEGORICAL_STRING_COLS:\n",
    "        categorical_string_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='string', max_tokens=5)\n",
    "        encoded_categorical_string_col = encoding_layer(categorical_string_col)\n",
    "        ALL_INPUTS.append(categorical_string_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_string_col)\n",
    "    \n",
    "        \n",
    "    #CREATE and COMPILE MODEL\n",
    "    all_features = tf.keras.layers.concatenate(ENCODED_FEATURES)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(ALL_INPUTS, output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    #TRAIN MODEL\n",
    "    history = model.fit(TRAIN_DS, epochs=10, validation_data=VALIDATION_DS)\n",
    "    \n",
    "    \n",
    "    #Define Bucket in GCS for Model Storage\n",
    "    BUCKET = 'gs://crazy-hippo-01/kubeflow_staging/earnings_model/models/'\n",
    "    \n",
    "    #Define Datasets Names\n",
    "    MODEL_PATH = BUCKET + 'earnings_model{}'.format(str(int(time.time())))\n",
    "    \n",
    "    MODEL.uri = MODEL_PATH \n",
    "    \n",
    "    \n",
    "    #Save model to Artifact Store for Project\n",
    "    model.save(MODEL.path)\n",
    "    \n",
    "    print('Model saved to: ' + MODEL.path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-municipality",
   "metadata": {},
   "source": [
    "#### Pipeline Step 3 - Evaluate Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "separated-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='evaluate_model.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas',\n",
    "                         'google-cloud-bigquery',\n",
    "                         'pyarrow', \n",
    "                         'gcsfs',\n",
    "                         'tensorflow'])\n",
    "def evaluate_validate_model(MODEL : Input[Model], \n",
    "                            TEST_DATA: Input[Dataset], \n",
    "                            pipeline:str, \n",
    "                            framework:str, \n",
    "                            metrics: Output[Metrics]) -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "          ('accuracy', float),\n",
    "          ('loss', float),\n",
    "      ]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    \n",
    "    #HELPER FUNCTION - TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('income_bracket')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #LOAD TRAINED MODEL FROM ARTIFACT STORE\n",
    "    reloaded_model = tf.keras.models.load_model(MODEL.path)\n",
    "    \n",
    "    #READ TESTING DATASET\n",
    "    TESTING_DATA = pd.read_csv(TEST_DATA.path)\n",
    "\n",
    "    #SET BATCG SIZE\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    #CALL HELPER FUNCTION TO CREATE TENSORFLOW DATASET\n",
    "    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #EVALUATE MODEL WITH TEST DATA\n",
    "    loss, accuracy = reloaded_model.evaluate(TESTING_DS)\n",
    "    \n",
    "    accuracy = float(accuracy)\n",
    "    loss = float(loss)\n",
    "    \n",
    "    #PRINT ACCURACY METRIC\n",
    "    print(\"Accuracy\", accuracy)\n",
    "    print(\"Loss\", loss)\n",
    "    \n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io    \n",
    "    \n",
    "    #Write Metrics to BigQuery Table for Validation and possible promotion to Deployment\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client(project='crazy-hippo-01')\n",
    "    \n",
    "    print('Sending Metrics into BigQuery')\n",
    "    \n",
    "    #Define DML Query to Insert Metrics into BugQuery\n",
    "    query = \"\"\"INSERT `crazy-hippo-01.census_data_us.model_metrics_history` (model_name, pipeline, framework, accuracy, loss)\n",
    "    VALUES (\"{}\", \"{}\", \"{}\", {}, {})  \n",
    "    \"\"\".format(MODEL.path, pipeline, framework, accuracy, loss)\n",
    "    \n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    \n",
    "    print('Metrics sent to BigQuery!')\n",
    "    \n",
    "    # Export two metrics\n",
    "    metrics.log_metric('accuracy', accuracy)\n",
    "    metrics.log_metric('loss', loss)\n",
    "\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    component_outputs = namedtuple('ComponentOutputs',\n",
    "        ['accuracy', 'loss'])\n",
    "        \n",
    "    return component_outputs(accuracy, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-hindu",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "supported-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='binaryclassmodel4',\n",
    "  description='Binary Classification Model with Tensorflow Deep Learning and Connected Pre-processing Layers'\n",
    ")\n",
    "def binary_classifier_earnings_v4(\n",
    "    pipeline: str = 'Tensorflow DL model with Integrated Preprocessing Version 4',\n",
    "    framework: str = 'Tensorflow',\n",
    "    input_path: str = 'crazy-hippo-01.census_data_us.census_raw'\n",
    "    ):\n",
    "   \n",
    "    first_step = read_transform_data(input_path)\n",
    "    #first_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "   \n",
    "    second_step = train_model(first_step.outputs['TRAINING_DATA'], \n",
    "                             first_step.outputs['TEST_DATA'], \n",
    "                             first_step.outputs['VALIDATION_DATA'])\n",
    "    #second_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    \n",
    "    third_step = evaluate_validate_model(second_step.outputs['MODEL'], \n",
    "                                        first_step.outputs['TEST_DATA'],\n",
    "                                       pipeline, framework)\n",
    "    #third_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-missile",
   "metadata": {},
   "source": [
    "### Compile and run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-shore",
   "metadata": {},
   "source": [
    "Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "limited-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=binary_classifier_earnings_v4, package_path=\"earnings_pipeline_ver4.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-wallet",
   "metadata": {},
   "source": [
    "Instantiate the API client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "incomplete-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-training",
   "metadata": {},
   "source": [
    "Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "falling-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = 'pipelines-vertex-ai@crazy-hippo-01.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"earnings_pipeline_ver4.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    service_account=SERVICE_ACCOUNT \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-sympathy",
   "metadata": {},
   "source": [
    "Alternate Test SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "generous-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = 'edit-rights@crazy-hippo-01.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-explanation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
