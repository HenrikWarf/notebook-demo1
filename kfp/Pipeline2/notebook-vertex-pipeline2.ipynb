{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spread-bridal",
   "metadata": {},
   "source": [
    "# Productionazing Machine Learning with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-attack",
   "metadata": {},
   "source": [
    "### Set up project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "apart-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  crazy-hippo-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"crazy-hippo-01\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-lancaster",
   "metadata": {},
   "source": [
    "### Define Current Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "sensitive-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp:  20210524115654\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print('Timestamp: ', TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-practice",
   "metadata": {},
   "source": [
    "### Create storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "alpine-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://crazy-vertex-ai-pipelines\"  # @param {type:\"string\"} \n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "plain-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"-vertex-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "continuing-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://crazy-vertex-ai-pipelines/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spectacular-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-strengthening",
   "metadata": {},
   "source": [
    "### Import Libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "broad-minute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://crazy-vertex-ai-pipelines/pipeline_root/crazy-hippo'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "USER = \"crazy-hippo\"  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "accepting-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-yield",
   "metadata": {},
   "source": [
    "### Define Pipeline Comnponents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-identity",
   "metadata": {},
   "source": [
    "#### Pipeline Step 1 - Extract and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "choice-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='read_transform_data.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'google-cloud-bigquery', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs', \n",
    "                             'sklearn'])\n",
    "def read_transform_data() -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "        ('training_data', str),\n",
    "        ('test_data', str),\n",
    "        ('validation_data', str),\n",
    "      ]):\n",
    "    \n",
    "    #Import libraries\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client(project='crazy-hippo-01')\n",
    "    \n",
    "    query = \"\"\"SELECT age, workclass, occupation, education_num, marital_status, capital_gain, income_bracket\n",
    "    FROM `crazy-hippo-01.census_data_us.census_raw` \n",
    "    \"\"\"\n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    #retry=bq_retry.DEFAULT_RETRY\n",
    "    df = job.to_dataframe()\n",
    "    \n",
    "    #df = pd.read_csv('gs://crazy-hippo-01/dataset/census_train.csv')\n",
    "    \n",
    "    #df = df[['age', 'workclass', 'occupation', 'education_num', 'marital_status', 'capital_gain', 'income_bracket']]\n",
    "    \n",
    "    #Drop null values in dataset\n",
    "    df = df.dropna()\n",
    "    \n",
    "    #Create training, test and validation datasets\n",
    "    train, test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "    train, val = train_test_split(train, test_size=0.20, random_state=42)\n",
    "\n",
    "    #Define Staging Bucket in GCS\n",
    "    BUCKET = 'gcs://crazy-hippo-01/kubeflow_staging/'\n",
    "    \n",
    "    #Define Datasets Names\n",
    "    TRAIN_DATA = BUCKET + 'datasets/training/training{}'.format(str(int(time.time())))  + '.csv'\n",
    "    TEST_DATA = BUCKET + 'datasets/testing/test{}'.format(str(int(time.time())))  + '.csv'\n",
    "    VALIDATION_DATA = BUCKET + 'datasets/validation/validation{}'.format(str(int(time.time())))  + '.csv'\n",
    "\n",
    "    #Write data to GCS Storage\n",
    "    train.to_csv(TRAIN_DATA, index=False, header=True)\n",
    "    test.to_csv(TEST_DATA, index=False, header=True)\n",
    "    val.to_csv(VALIDATION_DATA, index=False, header=True)\n",
    "\n",
    "    #Define outputs with namedtuple\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    return_values = namedtuple(\n",
    "      'ComponentOutputs',\n",
    "        ['training_data', 'test_data', 'validation_data'])\n",
    "        \n",
    "    return return_values(TRAIN_DATA, TEST_DATA, VALIDATION_DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-header",
   "metadata": {},
   "source": [
    "#### Pipeline Step 2 - Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "intimate-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='train_model.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs' , \n",
    "                             'google-cloud-bigquery-storage',\n",
    "                             'tensorflow'])\n",
    "def train_model(TRAIN_DATA:str, TEST_DATA:str, VALIDATION_DATA:str) -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "        ('model_path', str)\n",
    "      ]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental import preprocessing\n",
    "    \n",
    "    #VARIABLES AND TRAINING PARAMETERS\n",
    "    TRAINING_DATA = pd.read_csv(TRAIN_DATA)\n",
    "    TESTING_DATA = pd.read_csv(TEST_DATA)\n",
    "    VALIDATION_DATA = pd.read_csv(VALIDATION_DATA)\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    print(tf.__version__)\n",
    "    \n",
    "    print(TRAINING_DATA.head())\n",
    "    \n",
    "    print(TESTING_DATA.head())\n",
    "    \n",
    "    print(VALIDATION_DATA.head())\n",
    "    \n",
    "    #TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('income_bracket')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #NORMALIZATION FUNCTION\n",
    "    def helperfunc_get_normalization_layer(name, dataset):\n",
    "        # Create a Normalization layer for our feature.\n",
    "        normalizer = preprocessing.Normalization()\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the statistics of the data.\n",
    "        normalizer.adapt(feature_ds)\n",
    "\n",
    "        return normalizer\n",
    "    \n",
    "    #CATEGORY ENCODING FUNCTION\n",
    "    def helperfunc_get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "        # Create a StringLookup layer which will turn strings into integer indices\n",
    "        if dtype == 'string':\n",
    "            index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "        else:\n",
    "            index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the set of possible values and assign them a fixed integer index.\n",
    "        index.adapt(feature_ds)\n",
    "\n",
    "        # Create a Discretization for our integer indices.\n",
    "        encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = feature_ds.map(index)\n",
    "\n",
    "        # Learn the space of possible indices.\n",
    "        encoder.adapt(feature_ds)\n",
    "\n",
    "        # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "        # layer so we can use them, or include them in the functional model later.\n",
    "        return lambda feature: encoder(index(feature))\n",
    "    \n",
    "    #CREATE TENSORFLOW DATASETS\n",
    "    TRAIN_DS = helperfunc_create_dataset(TRAINING_DATA, batch_size=BATCH_SIZE)\n",
    "    VALIDATION_DS = helperfunc_create_dataset(VALIDATION_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #CREATE PREPROCESSING LAYERS\n",
    "    ALL_INPUTS = []\n",
    "    ENCODED_FEATURES = []\n",
    "\n",
    "    NUMERICAL = ['age' , 'capital_gain']\n",
    "    CATEGORICAL_INT_COLS = ['education_num']\n",
    "    CATEGORICAL_STRING_COLS = ['occupation', \n",
    "                               'workclass', \n",
    "                               'marital_status']\n",
    "    TARGET = ['income_bracket']\n",
    "    \n",
    "    # Numeric features.\n",
    "    for header in NUMERICAL:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = helperfunc_get_normalization_layer(header, TRAIN_DS)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        ALL_INPUTS.append(numeric_col)\n",
    "        ENCODED_FEATURES.append(encoded_numeric_col)\n",
    "        \n",
    "    # Categorical features encoded as integers.\n",
    "    for header in CATEGORICAL_INT_COLS:\n",
    "        categorical_int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='int64', max_tokens=5)\n",
    "        encoded_categorical_int_col = encoding_layer(categorical_int_col)\n",
    "        ALL_INPUTS.append(categorical_int_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_int_col)\n",
    "    \n",
    "    # Categorical features encoded as string.\n",
    "    for header in CATEGORICAL_STRING_COLS:\n",
    "        categorical_string_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='string', max_tokens=5)\n",
    "        encoded_categorical_string_col = encoding_layer(categorical_string_col)\n",
    "        ALL_INPUTS.append(categorical_string_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_string_col)\n",
    "    \n",
    "        \n",
    "    #CREATE and COMPILE MODEL\n",
    "    all_features = tf.keras.layers.concatenate(ENCODED_FEATURES)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(ALL_INPUTS, output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    #TRAIN MODEL\n",
    "    history = model.fit(TRAIN_DS, epochs=10, validation_data=VALIDATION_DS)\n",
    "    \n",
    "    \n",
    "    #Define Bucket in GCS for Model Storage\n",
    "    BUCKET = 'gs://crazy-hippo-01/kubeflow_staging/models/'\n",
    "    \n",
    "    #Define Datasets Names\n",
    "    MODEL_PATH = BUCKET + 'earnings_model{}'.format(str(int(time.time())))\n",
    "    \n",
    "    #Save model to Artifact Store for Project\n",
    "    model.save(MODEL_PATH)\n",
    "    \n",
    "    print('Model saved to: ' + MODEL_PATH)\n",
    "    \n",
    "    #Define outputs with namedtuple\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    return_values = namedtuple(\n",
    "      'ComponentOutputs',\n",
    "        ['model_path'])\n",
    "        \n",
    "    return return_values(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-wireless",
   "metadata": {},
   "source": [
    "#### Pipeline Step 3 - Evaluate Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "working-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='evaluate_model.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas',\n",
    "                         'google-cloud-bigquery',\n",
    "                         'pyarrow', \n",
    "                         'gcsfs',\n",
    "                         'tensorflow'])\n",
    "def evaluate_validate_model(saved_model:str, test_dataset:str, pipeline:str, framework:str, metrics: Output[Metrics]) -> NamedTuple(\n",
    "      'ComponentOutputs',\n",
    "      [\n",
    "          ('accuracy', float),\n",
    "          ('loss', float),\n",
    "      ]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    \n",
    "    print(saved_model, test_dataset, pipeline, framework)\n",
    "    \n",
    "    #HELPER FUNCTION - TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('income_bracket')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #LOAD TRAINED MODEL FROM ARTIFACT STORE\n",
    "    reloaded_model = tf.keras.models.load_model(saved_model)\n",
    "    \n",
    "    #READ TESTING DATASET\n",
    "    TESTING_DATA = pd.read_csv(test_dataset)\n",
    "\n",
    "    #SET BATCG SIZE\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    #CALL HELPER FUNCTION TO CREATE TENSORFLOW DATASET\n",
    "    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #EVALUATE MODEL WITH TEST DATA\n",
    "    loss, accuracy = reloaded_model.evaluate(TESTING_DS)\n",
    "    \n",
    "    accuracy = float(accuracy)\n",
    "    loss = float(loss)\n",
    "    \n",
    "    #PRINT ACCURACY METRIC\n",
    "    print(\"Accuracy\", accuracy)\n",
    "    print(\"Loss\", loss)\n",
    "    \n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io    \n",
    "    \n",
    "    #Write Metrics to BigQuery Table for Validation and possible promotion to Deployment\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client(project='crazy-hippo-01')\n",
    "    \n",
    "    print('Sending Metrics into BigQuery')\n",
    "    \n",
    "    #Define DML Query to Insert Metrics into BugQuery\n",
    "    query = \"\"\"INSERT `crazy-hippo-01.census_data_us.model_metrics_history` (model_name, pipeline, framework, accuracy, loss)\n",
    "    VALUES (\"{}\", \"{}\", \"{}\", {}, {})  \n",
    "    \"\"\".format(saved_model, pipeline, framework, accuracy, loss)\n",
    "    \n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    \n",
    "    print('Metrics sent to BigQuery!')\n",
    "    \n",
    "    # Export two metrics\n",
    "    metrics.log_metric('accuracy', accuracy)\n",
    "    metrics.log_metric('loss', loss)\n",
    "\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    component_outputs = namedtuple('ComponentOutputs',\n",
    "        ['accuracy', 'loss'])\n",
    "        \n",
    "    return component_outputs(accuracy, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-effectiveness",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "sublime-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='binaryclassmodel3',\n",
    "  description='Binary Classification Model with Tensorflow Deep Learning and Connected Pre-processing Layers'\n",
    ")\n",
    "def binary_classifier_earnings_v3(\n",
    "    pipeline: str = 'Tensorflow DL model with Integrated Preprocessing Version 1',\n",
    "    framework: str = 'Tensorflow'\n",
    "    ):\n",
    "   \n",
    "    first_step = read_transform_data()\n",
    "    #first_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "   \n",
    "    second_step = train_model(first_step.outputs['training_data'], \n",
    "                             first_step.outputs['test_data'], \n",
    "                             first_step.outputs['validation_data'])\n",
    "    #second_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    \n",
    "    third_step = evaluate_validate_model(second_step.outputs['model_path'], \n",
    "                                        first_step.outputs['test_data'],\n",
    "                                       pipeline, framework)\n",
    "    #third_step.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-transaction",
   "metadata": {},
   "source": [
    "### Compile and run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-westminster",
   "metadata": {},
   "source": [
    "Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "western-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=binary_classifier_earnings_v3, package_path=\"earnings_pipeline_ver3.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-metabolism",
   "metadata": {},
   "source": [
    "Instantiate the API client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "configured-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-mills",
   "metadata": {},
   "source": [
    "Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "judicial-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = 'pipelines-vertex-ai@crazy-hippo-01.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "stupid-designer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/binaryclassmodel3-20210524074227?project=crazy-hippo-01\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"earnings_pipeline_ver3.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    service_account=SERVICE_ACCOUNT \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-export",
   "metadata": {},
   "source": [
    "Alternate Test SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = 'owner-sa@crazy-hippo-01.iam.gserviceaccount.com'"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
