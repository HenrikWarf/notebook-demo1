{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biblical-diving",
   "metadata": {},
   "source": [
    "# AI Platform Pipeline with TFX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-exemption",
   "metadata": {},
   "source": [
    "### Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiplatform.pipelines import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "developing-monte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-aiplatform-pipelines/releases/20210209/aiplatform_pipelines_client-0.1.0.caip20210209-py3-none-any.whl...\n",
      "/ [1 files][ 21.9 KiB/ 21.9 KiB]                                                \n",
      "Operation completed over 1 objects/21.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Get the AI Platform client library from restricted bucket\n",
    "!gsutil cp gs://cloud-aiplatform-pipelines/releases/20210209/aiplatform_pipelines_client-0.1.0.caip20210209-py3-none-any.whl .  \n",
    "# Get the Metadata SDK to query the produced metadata.\n",
    "!gsutil cp gs://cloud-aiplatform-metadata/sdk/google-cloud-aiplatform-metadata-0.0.1.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install both libraries\n",
    "!python3 -m pip install google-cloud-aiplatform\n",
    "!python3 -m pip install kfp==1.4 google-cloud-aiplatform-metadata-0.0.1.tar.gz aiplatform_pipelines_client-0.1.0.caip20210209-py3-none-any.whl --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "partial-singles",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Text, List\n",
    "import absl\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "#from aiplatform.pipelines import client\n",
    "\n",
    "from tfx.components.example_gen.import_example_gen.component import ImportExampleGen\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import InfraValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import ResolverNode\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "from tfx.orchestration import metadata\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "from tfx.dsl.experimental import latest_artifacts_resolver ## demo\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver ## demo\n",
    "from tfx.orchestration import pipeline as tfx_pipeline\n",
    "from tfx.orchestration.local.local_dag_runner import LocalDagRunner\n",
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.types import standard_artifacts\n",
    "from tfx.types import channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "artistic-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.2\n",
      "TFX version: 0.27.0\n"
     ]
    }
   ],
   "source": [
    "# Check Versions of Pipeline\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(__import__('tfx.version').__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-hanging",
   "metadata": {},
   "source": [
    "### Set Environment Variables for Local execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sublime-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID         = 'crazy-hippo-01'\n",
    "REGION             = 'us-central1'\n",
    "API_KEY            = ''\n",
    "PIPELINE_NAME      = 'earnings'\n",
    "PIPELINE_ROOT      = 'gs://crazy-hippo-01/tfx/binary_classification/pipeline'\n",
    "TRANSFORM_FILE     = 'transform.py'\n",
    "#TRANSFORM_FILE    = 'gs://crazy-hippo-01/tfx/binary_classification/transform.py'\n",
    "TRAINER_FILE       = 'trainer.py'\n",
    "#TRAINER_FILE      = 'gs://crazy-hippo-01/tfx/binary_classification/trainer.py'\n",
    "RAW_DATA           = \"gs://crazy-hippo-01/tfx/binary_classification/raw/\"\n",
    "SERVING_MODEL_DIR = 'gs://crazy-hippo-01/tfx/binary_classification/serving_model/'\n",
    "METADATA_PATH     = os.path.join('.', 'tfx_metadata', PIPELINE_NAME, 'metadata.db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-conviction",
   "metadata": {},
   "source": [
    "### Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "former-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfx_pipeline(\n",
    "    pipeline_name: Text, input_dir: Text, metadata_connection_config: Optional[metadata_store_pb2.ConnectionConfig] = None\n",
    "    ):\n",
    "   \n",
    "    # Output 2 splits: train:eval=3:1.\n",
    "    example_gen = CsvExampleGen(input_base=RAW_DATA)\n",
    "    \n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        infer_feature_shape=True)\n",
    "\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema'])\n",
    "\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=TRANSFORM_FILE)\n",
    "    \n",
    "    # Fetch the latest trained model under the same context for warm-starting.\n",
    "    latest_model_resolver = ResolverNode(\n",
    "        instance_name='latest_model_resolver',\n",
    "        resolver_class=latest_artifacts_resolver.LatestArtifactsResolver,\n",
    "        model=channel.Channel(type=standard_artifacts.Model))\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        module_file=TRAINER_FILE,\n",
    "        custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=3000),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=3000))\n",
    "    \n",
    "    # Get the latest blessed model for model validation.\n",
    "    model_resolver = ResolverNode(\n",
    "        instance_name='latest_blessed_model_resolver',\n",
    "        resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "        model=channel.Channel(type=standard_artifacts.Model),\n",
    "        model_blessing=channel.Channel(type=standard_artifacts.ModelBlessing))\n",
    "\n",
    "    # Set the TFMA config for Model Evaluation and Validation.\n",
    "    eval_config = tfma.EvalConfig(\n",
    "       model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "       slicing_specs=[tfma.SlicingSpec()],\n",
    "       metrics_specs=[\n",
    "           tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(\n",
    "                  class_name='SparseCategoricalAccuracy',\n",
    "                  threshold=tfma.MetricThreshold(\n",
    "                      # Accept models only if SparseCategoricalAccuracy > 0.8\n",
    "                      value_threshold=tfma.GenericValueThreshold(\n",
    "                          lower_bound={'value': 0.8}),\n",
    "                      # TODO: modify this\n",
    "                      change_threshold=tfma.GenericChangeThreshold(\n",
    "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                          absolute={'value': -1e-2})))\n",
    "          ])\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        # Change threshold will be ignored if there is no baseline (first run).\n",
    "        eval_config=eval_config)\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        #model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=SERVING_MODEL_DIR)))\n",
    "\n",
    "    components=[\n",
    "        example_gen, statistics_gen, schema_gen, example_validator, \n",
    "        transform, trainer, pusher\n",
    "    ]\n",
    "\n",
    "    return tfx_pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        components=components,\n",
    "        metadata_connection_config=metadata_connection_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-enzyme",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "multiple-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:From /opt/conda/lib/python3.7/site-packages/tfx/components/common_nodes/resolver_node.py:74: The name tfx.components.common_nodes.resolver_node.ResolverNode is deprecated. Please use tfx.dsl.components.common.resolver.ResolverNode instead.\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n"
     ]
    }
   ],
   "source": [
    "mypipeline = create_tfx_pipeline(\n",
    "          pipeline_name=PIPELINE_NAME,\n",
    "          input_dir=RAW_DATA,\n",
    "          metadata_connection_config=metadata.sqlite_metadata_connection_config(METADATA_PATH)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-section",
   "metadata": {},
   "source": [
    "#### Run Pipeline in Local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "LocalDagRunner().run(mypipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-eligibility",
   "metadata": {},
   "source": [
    "## Run Pipeline in exernal Runners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-dairy",
   "metadata": {},
   "source": [
    "### Deploying pipeline to Kubeflow Kluster in GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user kfp\n",
    "!pip install --upgrade --user tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "resistant-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 50.1M  100 50.1M    0     0  94.8M      0 --:--:-- --:--:-- --:--:-- 94.6M\n"
     ]
    }
   ],
   "source": [
    "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mv skaffold /home/jupyter/.local/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "noble-movie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "czech-witness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.27.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "binary-serbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCP project ID:crazy-hippo-01\n"
     ]
    }
   ],
   "source": [
    "# Read GCP project id from env.\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GCP_PROJECT_ID=shell_output[0]\n",
    "print(\"GCP project ID:\" + GCP_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "compact-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT      = '18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com'\n",
    "PIPELINE_PATH = 'kubeflow_runner.py'\n",
    "PACKAGE_PATH  = 'pipeline.json'\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/' + GCP_PROJECT_ID + '/tfx-pipeline'\n",
    "\n",
    "os.environ['PACKAGE_PATH'] = PACKAGE_PATH\n",
    "os.environ['ENDPOINT'] = ENDPOINT\n",
    "os.environ['PIPELINE_PATH'] = PIPELINE_PATH\n",
    "os.environ['CUSTOM_TFX_IMAGE'] = CUSTOM_TFX_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "recognized-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:57:56.021199: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "kubeflow runner not found in dsl.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broke-penny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:47:06.050666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "CLI\n",
      "Compiling pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "kubeflow runner not found in dsl.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --pipeline_path=kubeflow_runner.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run create --pipeline-name=$PIPELINE_NAME --endpoint=$ENDPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-ready",
   "metadata": {},
   "source": [
    "### Creating Output File of the Pipeline and Run in AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "intimate-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://transform.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \n",
      "Operation completed over 1 objects/2.8 KiB.                                      \n",
      "Copying file://trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  5.2 KiB/  5.2 KiB]                                                \n",
      "Operation completed over 1 objects/5.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp transform.py gs://crazy-hippo-01/tfx/binary_classification\n",
    "!gsutil cp trainer.py gs://crazy-hippo-01/tfx/binary_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "younger-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID         = 'crazy-hippo-01'\n",
    "REGION             = 'us-central1'\n",
    "PIPELINE_NAME      = 'earnings'\n",
    "PIPELINE_ROOT      = 'gs://crazy-hippo-01/tfx/binary_classification/pipeline'\n",
    "TRANSFORM_FILE     = 'gs://crazy-hippo-01/tfx/binary_classification/transform.py'\n",
    "TRAINER_FILE       = 'gs://crazy-hippo-01/tfx/binary_classification/trainer.py'\n",
    "RAW_DATA           = \"gs://crazy-hippo-01/tfx/binary_classification/raw/\"\n",
    "SERVING_MODEL_DIR  = 'gs://crazy-hippo-01/tfx/binary_classification/serving_model/'\n",
    "METADATA_PATH      = os.path.join('.', 'tfx_metadata', PIPELINE_NAME, 'metadata.db')\n",
    "API_KEY = 'AIzaSyBZQOYPxdfgGzmc5qMr6-LMFK8RHH9RSMs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instructional-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'displayName': 'earnings',\n",
       " 'pipelineSpec': {'tasks': [{'executorLabel': 'CsvExampleGen_executor',\n",
       "    'outputs': {'artifacts': {'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: int\\n    description: Span for an artifact.\\n  version:\\n    type: int\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}},\n",
       "    'cachingOptions': {},\n",
       "    'taskInfo': {'name': 'CsvExampleGen'},\n",
       "    'inputs': {'parameters': {'output_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}'}}},\n",
       "      'input_base': {'runtimeValue': {'constantValue': {'stringValue': 'gs://crazy-hippo-01/tfx/binary_classification/raw/'}}},\n",
       "      'output_data_format': {'runtimeValue': {'constantValue': {'intValue': '6'}}},\n",
       "      'input_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}'}}}}}},\n",
       "   {'inputs': {'artifacts': {'examples': {'outputArtifactKey': 'examples',\n",
       "       'producerTask': 'CsvExampleGen'}},\n",
       "     'parameters': {'exclude_splits': {'runtimeValue': {'constantValue': {'stringValue': '[]'}}}}},\n",
       "    'dependentTasks': ['CsvExampleGen'],\n",
       "    'cachingOptions': {},\n",
       "    'outputs': {'artifacts': {'statistics': {'artifactType': {'instanceSchema': 'title: tfx.ExampleStatistics\\ntype: object\\nproperties:\\n  span:\\n    type: int\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}},\n",
       "    'taskInfo': {'name': 'StatisticsGen'},\n",
       "    'executorLabel': 'StatisticsGen_executor'},\n",
       "   {'inputs': {'parameters': {'exclude_splits': {'runtimeValue': {'constantValue': {'stringValue': '[]'}}},\n",
       "      'infer_feature_shape': {'runtimeValue': {'constantValue': {'intValue': '1'}}}},\n",
       "     'artifacts': {'statistics': {'producerTask': 'StatisticsGen',\n",
       "       'outputArtifactKey': 'statistics'}}},\n",
       "    'executorLabel': 'SchemaGen_executor',\n",
       "    'dependentTasks': ['StatisticsGen'],\n",
       "    'taskInfo': {'name': 'SchemaGen'},\n",
       "    'outputs': {'artifacts': {'schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'cachingOptions': {}},\n",
       "   {'inputs': {'artifacts': {'statistics': {'outputArtifactKey': 'statistics',\n",
       "       'producerTask': 'StatisticsGen'},\n",
       "      'schema': {'outputArtifactKey': 'schema', 'producerTask': 'SchemaGen'}},\n",
       "     'parameters': {'exclude_splits': {'runtimeValue': {'constantValue': {'stringValue': '[]'}}}}},\n",
       "    'taskInfo': {'name': 'ExampleValidator'},\n",
       "    'outputs': {'artifacts': {'anomalies': {'artifactType': {'instanceSchema': 'title: tfx.ExampleAnomalies\\ntype: object\\nproperties:\\n  span:\\n    type: int\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}},\n",
       "    'executorLabel': 'ExampleValidator_executor',\n",
       "    'cachingOptions': {},\n",
       "    'dependentTasks': ['SchemaGen', 'StatisticsGen']},\n",
       "   {'taskInfo': {'name': 'Transform'},\n",
       "    'cachingOptions': {},\n",
       "    'dependentTasks': ['CsvExampleGen', 'SchemaGen'],\n",
       "    'inputs': {'parameters': {'custom_config': {'runtimeValue': {'constantValue': {'stringValue': 'null'}}},\n",
       "      'module_file': {'runtimeValue': {'constantValue': {'stringValue': 'gs://crazy-hippo-01/tfx/binary_classification/transform.py'}}},\n",
       "      'force_tf_compat_v1': {'runtimeValue': {'constantValue': {'intValue': '1'}}}},\n",
       "     'artifacts': {'schema': {'producerTask': 'SchemaGen',\n",
       "       'outputArtifactKey': 'schema'},\n",
       "      'examples': {'producerTask': 'CsvExampleGen',\n",
       "       'outputArtifactKey': 'examples'}}},\n",
       "    'outputs': {'artifacts': {'transformed_examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: int\\n    description: Span for an artifact.\\n  version:\\n    type: int\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}},\n",
       "      'updated_analyzer_cache': {'artifactType': {'instanceSchema': 'title: tfx.TransformCache\\ntype: object\\nproperties:\\n'}},\n",
       "      'transform_graph': {'artifactType': {'instanceSchema': 'title: tfx.TransformGraph\\ntype: object\\nproperties:\\n'}}}},\n",
       "    'executorLabel': 'Transform_executor'},\n",
       "   {'cachingOptions': {},\n",
       "    'executorLabel': 'Trainer_executor',\n",
       "    'dependentTasks': ['SchemaGen', 'Transform'],\n",
       "    'taskInfo': {'name': 'Trainer'},\n",
       "    'inputs': {'artifacts': {'examples': {'outputArtifactKey': 'transformed_examples',\n",
       "       'producerTask': 'Transform'},\n",
       "      'transform_graph': {'outputArtifactKey': 'transform_graph',\n",
       "       'producerTask': 'Transform'},\n",
       "      'schema': {'producerTask': 'SchemaGen', 'outputArtifactKey': 'schema'}},\n",
       "     'parameters': {'custom_config': {'runtimeValue': {'constantValue': {'stringValue': 'null'}}},\n",
       "      'train_args': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"num_steps\": 3000\\n}'}}},\n",
       "      'eval_args': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"num_steps\": 3000\\n}'}}},\n",
       "      'module_file': {'runtimeValue': {'constantValue': {'stringValue': 'gs://crazy-hippo-01/tfx/binary_classification/trainer.py'}}}}},\n",
       "    'outputs': {'artifacts': {'model_run': {'artifactType': {'instanceSchema': 'title: tfx.ModelRun\\ntype: object\\nproperties:\\n'}},\n",
       "      'model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\nproperties:\\n'}}}}},\n",
       "   {'inputs': {'artifacts': {'model': {'producerTask': 'Trainer',\n",
       "       'outputArtifactKey': 'model'}},\n",
       "     'parameters': {'push_destination': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"gs://crazy-hippo-01/tfx/binary_classification/serving_model/\"\\n  }\\n}'}}},\n",
       "      'custom_config': {'runtimeValue': {'constantValue': {'stringValue': 'null'}}}}},\n",
       "    'executorLabel': 'Pusher_executor',\n",
       "    'dependentTasks': ['Trainer'],\n",
       "    'cachingOptions': {},\n",
       "    'taskInfo': {'name': 'Pusher'},\n",
       "    'outputs': {'artifacts': {'pushed_model': {'artifactType': {'instanceSchema': 'title: tfx.PushedModel\\ntype: object\\nproperties:\\n'}}}}}],\n",
       "  'schemaVersion': '1.0.0',\n",
       "  'pipelineInfo': {'name': 'earnings'},\n",
       "  'deploymentSpec': {'executors': {'Trainer_executor': {'container': {'image': 'gcr.io/tfx-oss-public/tfx:0.27.0',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'tfx.components.trainer.executor.GenericExecutor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'ExampleValidator_executor': {'container': {'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'image': 'gcr.io/tfx-oss-public/tfx:0.27.0',\n",
       "      'args': ['--executor_class_path',\n",
       "       'tfx.components.example_validator.executor.Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'StatisticsGen_executor': {'container': {'args': ['--executor_class_path',\n",
       "       'tfx.components.statistics_gen.executor.Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'image': 'gcr.io/tfx-oss-public/tfx:0.27.0'}},\n",
       "    'SchemaGen_executor': {'container': {'image': 'gcr.io/tfx-oss-public/tfx:0.27.0',\n",
       "      'args': ['--executor_class_path',\n",
       "       'tfx.components.schema_gen.executor.Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}},\n",
       "    'CsvExampleGen_executor': {'container': {'args': ['--executor_class_path',\n",
       "       'tfx.components.example_gen.csv_example_gen.executor.Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'image': 'gcr.io/tfx-oss-public/tfx:0.27.0',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'lifecycle': {'preCacheCheck': {'command': ['python',\n",
       "         '-m',\n",
       "         'tfx.orchestration.kubeflow.v2.file_based_example_gen.driver'],\n",
       "        'args': ['--json_serialized_invocation_args', '{{$}}']}}}},\n",
       "    'Pusher_executor': {'container': {'image': 'gcr.io/tfx-oss-public/tfx:0.27.0',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'tfx.components.pusher.executor.Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}']}},\n",
       "    'Transform_executor': {'container': {'image': 'gcr.io/tfx-oss-public/tfx:0.27.0',\n",
       "      'args': ['--executor_class_path',\n",
       "       'tfx.components.transform.executor.Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}'],\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}}}},\n",
       "  'sdkVersion': '0.27.0'},\n",
       " 'labels': {'tfx_runner': 'kubeflow_v2',\n",
       "  'tfx_py_version': '3-7',\n",
       "  'tfx_version': '0-27-0'},\n",
       " 'runtimeConfig': {'gcsOutputDirectory': 'gs://crazy-hippo-01/tfx/binary_classification/pipeline'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(\n",
    "    project_id=PROJECT_ID,\n",
    "    display_name=PIPELINE_NAME)\n",
    "\n",
    "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config=config,\n",
    "    output_filename='pipeline01.json')\n",
    "\n",
    "runner.run(pipeline=mypipeline, write_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "august-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earnings1617189412\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from aiplatform.pipelines import client\n",
    "\n",
    "api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n",
    "DISPLAY_NAME = 'earnings{}'.format(str(int(time.time())))\n",
    "print(DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aging-liquid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/ai/platform/pipelines/runs/earnings1617189412?e=CaipPipelinesAlphaLaunch::CaipPipelinesAlphaEnabled,BackendzOverridingLaunch::BackendzOverridingEnabled,CloudAiLaunch::CloudAiEnabled&project=crazy-hippo-01\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = api_client.create_run_from_job_spec(\n",
    "          job_spec_path='pipeline01.json',\n",
    "          name = DISPLAY_NAME\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-token",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = api_client.create_run_from_job_spec(\n",
    "          job_spec_path='pipeline01.json',\n",
    "          name = DISPLAY_NAME,\n",
    "          parameter_values={'gcp_project_id': '{}'.format(PROJECT_ID),\n",
    "                           'dataset_display_name': DISPLAY_NAME,\n",
    "                            'endpoint_display_name': DISPLAY_NAME,\n",
    "                            'training_display_name': DISPLAY_NAME,\n",
    "                            'thresholds_dict_str': '{\"meanAbsoluteError\": 470}',\n",
    "                            'use_dataflow': 'true',\n",
    "                            'data_dir': DATA_DIR, 'bigquery_uri': BIGQUERY_URI\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-woman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='18bbb292bf653151-dot-us-central2.pipelines.googleusercontent.com')\n",
    "client.list_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n",
    "client.create_run_from_job_spec('pipeline.json')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
